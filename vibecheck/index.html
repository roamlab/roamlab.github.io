<!DOCTYPE HTML>
<html>
	<head>
		<title>VibeCheck</title>
		<meta charset="utf-8" />
		 <meta name="viewport" content="width=1000">
		<link rel="stylesheet" href="assets/css/main.css" />
		<link rel="stylesheet" href="assets/css/styles.css">
		<link rel="icon" type="image/jpg" href="assets/images/emoji.jpg">
		 <script>
          (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
          (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
          })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
          ga('create', 'UA-89797207-1', 'auto');
          ga('send', 'pageview');
		</script>

		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

	    <meta property="og:type"          content="website" />
	    <meta property="og:title"         content="VibeCheck" />
	    <meta property="og:description"   content=""/>

	</head>

	<body id="top">
		<!-- Main -->
			<div id="main" style="padding-bottom:1em; padding-top: 5em; width: 60em; max-width: 70em; margin-left: auto; margin-right: auto;">
					<section id="four">
						<h1 style="text-align: center; margin-bottom: 0; color: #4e79a7; font-size: 180%; margin-bottom: 1em;">
							VibeCheck: Using Active Acoustic Tactile Sensing<br>for Contact-Rich Manipulation
						</h1>

						<!-- Author Section -->
						<div class="author-container">
							<div class="author-grid">
								<a href="https://aaronzkd.github.io/KaidiZhang.web/">Kaidi Zhang*</a>
								<a href="https://do-gon.github.io/">Do-Gon Kim*</a>
								<a href="https://www.ricandrobots.com/">Eric T. Chang*</a>
								<a href="https://wilson20010327.github.io/">Hua-Hsuan Liang</a>
								<a href="https://zhanpenghe.github.io/">Zhanpeng He</a>
							</div>
							<div class="author-grid">
								<a href="https://kathrynlampo.com/">Kathryn Lampo</a>
								<a href="https://www.philippewu.me/">Philippe Wu</a>
								<a href="https://www.ee.columbia.edu/ioannis-john-kymissis">Ioannis Kymissis</a>
								<a href="https://roam.me.columbia.edu/people/matei-ciocarlie">Matei Ciocarlie</a>
							</div>
							<p class="equal-contribution">*equal contribution</p>
							<p class="affiliation">Columbia University</p>
						</div>

						<div class="image fit" style="text-align: center;">
							<img src="images/teaser.png" alt="" style="max-width: 80%; display: block; margin: auto;" />
						</div>
						<p style="text-align: justify;">
							The acoustic response of an object can reveal a lot about its global state, for example its material properties or the extrinsic contacts it is making with the world. In this work, we build an active acoustic sensing gripper equipped with two piezoelectric fingers: one for generating signals, the other for receiving them. By sending an acoustic vibration from one finger to the other <em>through</em> an object, we gain insight into an object's acoustic properties and contact state. We use this system to classify objects, estimate grasping position, estimate poses of internal structures, and classify the types of extrinsic contacts an object is making with the environment. Using our contact type classification model, we tackle a standard long-horizon manipulation problem: peg insertion. We use a simple simulated transition model based on the performance of our sensor to train an imitation learning policy that is robust to imperfect predictions from the classifier. We finally demonstrate the policy on a UR5 robot with active acoustic sensing as the <em>only</em> feedback.
						</p>



						
						<hr style="margin-top: 0em;">
						<h3>Paper</h3>
						<p style="margin-bottom: 0.5em;">
						Latest version: 
						<a href="https://arxiv.org/abs/2504.15535" target="_blank">arXiv</a>.<br> 
						<!-- <a href="images/iros2025_acoustic.pdf" target="_blank">PDF</a>.<br> -->
						IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) 2025<br>
						</p>
						<!-- <div class="paper-thumb">
						<a href="images/iros2025_acoustic.pdf" target="_blank">
							<img 
							src="images/vibecheck_overview.png" 
							/>
						</a>
						</div> -->

						<hr>
						<div class="row">
						<div class="12u$ 12u$(xsmall)" style="text-align: center;">
							<h3>Supplementary Video</h3>
							<div class="video-container">
							<iframe
								id="match-video"
								src="images/videos/vibecheck supplemental video compressed.mp4"
								frameborder="0"
								allowfullscreen>
							</iframe>
							</div>
						</div>
						</div>



						
						<hr>
						<h3>Team</h3>
						<section>
							<div class="team-container">
								<div class="team-member">
									<a href="https://aaronzkd.github.io/KaidiZhang.web/">
										<img src="images/profile/kaidi.jpg" alt="Kaidi Zhang">
										<p>Kaidi Zhang</p>
									</a>
								</div>
								<div class="team-member">
									<a href="https://do-gon.github.io/">
										<img src="images/profile/dogon.jpg" alt="Do-Gon Kim">
										<p>Do-Gon Kim</p>
									</a>
								</div>
								<div class="team-member">
									<a href="https://www.ricandrobots.com/">
										<img src="images/profile/propic.png" alt="Eric Chang">
										<p>Eric Chang</p>
									</a>
								</div>
								<div class="team-member">
									<a href="https://wilson20010327.github.io/">
										<img src="images/profile/wilson.jpg" alt="Hua-Hsuan Liang">
										<p>Hua-Hsuan Liang</p>
									</a>
								</div>
								<div class="team-member">
									<a href="https://zhanpenghe.github.io/">
										<img src="https://zhanpenghe.github.io/resources/headshot.jpg" alt="Zhanpeng He">
										<p>Zhanpeng He</p>
									</a>
								</div>
								<div class="team-member">
									<a href="https://kathrynlampo.com/">
										<img src="images/profile/kate.jpg" alt="Kathryn Lampo">
										<p>Kathryn Lampo</p>
									</a>
								</div>
								<div class="team-member">
									<a href="https://www.philippewu.me/">
										<img src="images/profile/philippe.jpg" alt="Philippe Wu">
										<p>Philippe Wu</p>
									</a>
								</div>
								<div class="team-member">
									<a href="https://www.ee.columbia.edu/ioannis-john-kymissis">
										<img src="images/profile/john.jpg" alt="Ioannis Kymissis">
										<p>Ioannis Kymissis</p>
									</a>
								</div>
								<div class="team-member">
									<a href="https://roam.me.columbia.edu/people/matei-ciocarlie">
										<img src="images/profile/matei.jpg" alt="Matei Ciocarlie">
										<p>Matei Ciocarlie</p>
									</a>
								</div>
							</div>
						</section>
						
						<a href="https://roam.me.columbia.edu/">ROAM Lab</a>, Columbia University
						<br>
						<a href="https://kymissis.columbia.edu/">CLUE Lab</a>, Columbia University
						
						<hr/>
						<div class="row" style="margin-top: 0em; margin-bottom: -1em">
							<div class="12u$ 12u$(xsmall)">
							  <h3>BibTeX</h3>
							  <pre><code>@inproceedings{zhang2025vibecheck,
	title={VibeCheck: Using Active Acoustic Tactile Sensing for Contact-Rich Manipulation},
	author={Zhang, Kaidi and Kim, Do-Gon and Chang, Eric T. and Liang, Hua-Hsuan and He, Zhanpeng and Lampo, Kathryn and Wu, Philippe and Kymissis, Ioannis and Ciocarlie, Matei},
	booktitle={Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
	year={2025}
}</code></pre>
							</div>
						</div>




						<hr/>
						<h3>Hardware Design</h3>
						<p style="text-align: justify; margin-bottom: 0;">
							We built on previous work and recreate a hardware platform. The sensor housing ① is a 3D-printed Formlabs Clear V4 resin mount that aligns the piezoelectric disk ②, which is backed by high-density foam tape to ensure even pressure distribution and strong coupling. A Sorbothane pad ③ provides elastomeric isolation, suppressing unwanted vibrations from gripper motor and UR5 robot, while polyethylene strips ④ on the housing fingertips prevents slippage during excitation. On the speaker side ⑤, a Teensy-controlled audio adapter shield and inverting amplifier circuit drive the piezoelectric transducer to generate the swept-sine excitation. On the receiver side ⑥, an identical piezoelectric transducer captures the vibration response and feeds the signal directly into the Teensy’s ADC at 44.1 kHz.
						</p>

						<div class="video fit" style="text-align: center; margin-top: 0;">
						<video autoplay loop muted playsinline preload="auto" 
								style="max-width: 100%; display: block; margin: auto;">
							<source src="images/hardware/vibecheck_animation.mp4" type="video/mp4">
							Your browser does not support the video tag.
						</video>
						</div>




						<hr/>
						<h3>Task Experiments</h3>

						<h4>1) Object Classification</h4>
						<div style="display: flex; align-items: center; gap: 1em; margin-bottom: 1.5em;">
						<div style="flex: 0 0 250px;">
							<img src="images/task_experiment/object_classification.png" 
								alt="Object Classification Results"
								style="width: 100%; height: auto; border: 2px solid #ddd; border-radius: 6px;">
						</div>
						<div style="flex: 1; text-align: justify;">
							<p>
								<em>Goal:</em> Identify object type (material / hollow vs. solid) from a single grasp.<br>
								<em>Result:</em> 100% accuracy in-distribution across 9 classes, with generalization to new surfaces and orientations. Best trade-off used kernel PCA with 5 principal components (≈91% variance for best out-of-distribution), though even 3 principal components achieved 100% in-distribution.
							</p>
						</div>
						</div>

						<h4>2) Grasping Position Classification</h4>
						<div style="display: flex; align-items: center; gap: 1em; margin-bottom: 1.5em;">
						<div style="flex: 0 0 250px;">
							<img src="images/task_experiment/grasping_position.png" 
								alt="Grasping Position Classification Results"
								style="width: 100%; height: auto; border: 2px solid #ddd; border-radius: 6px;">
						</div>
						<div style="flex: 1; text-align: justify;">
							<p>
								<em>Goal:</em> Estimate where along a rod the grasp occurs (edge / quarter / center).<br>
								<em>Result:</em> 100% accuracy in-distribution and strong under out-of-distribution conditions. Using ~10 principal components (≈90% variance) balanced generalization while avoiding overfitting.
							</p>
						</div>
						</div>

						<h4>3) Pose Estimation from Internal Structure</h4>
						<div style="display: flex; align-items: center; gap: 1em; margin-bottom: 1.5em;">
						<div style="flex: 0 0 250px;">
							<img src="images/task_experiment/internal_structure.png" 
								alt="Pose Estimation from Internal Structure"
								style="width: 100%; height: auto; border: 2px solid #ddd; border-radius: 6px;">
						</div>
						<div style="flex: 1; text-align: justify;">
							<p>
								<em>Goal:</em> Estimate an object’s orientation from transmitted acoustic signals.<br>
								<em>Result:</em> Regression achieved ~20° RMSE overall, with smaller errors at most angles. 5 principal components (≈87% variance) gave the best trade-off.
							</p>
						</div>
						</div>

						<h4>4) Contact Type Classification</h4>
						<div style="display: flex; align-items: center; gap: 1em; margin-bottom: 1.5em;">
						<div style="flex: 0 0 250px;">
							<img src="images/task_experiment/contact_type.png" 
								alt="Contact Type Classification Results"
								style="width: 100%; height: auto; border: 2px solid #ddd; border-radius: 6px;">
						</div>
						<div style="flex: 1; text-align: justify;">
							<p>
								<em>Goal:</em> Predict the extrinsic contact state of a rod (diagonal / line / in-hole).<br>
								<em>Result:</em> Achieved 95% in-distribution accuracy; the most difficult cases occur near diagonal–line boundaries. High-dimensional embeddings (~500 PCs) produced the strongest results.
							</p>
						</div>
						</div>




						<hr/>
						<h3>Data Collection Methods</h3>
						<p style="text-align: justify;">
							We use active acoustic sensing with a linear swept-sine excitation, sampled at 44.1&nbsp;kHz: 20&nbsp;Hz → 20&nbsp;kHz over 1&nbsp;s. Within each grasp, we collect 5 sweeps before regrasping.
						</p>

						<h4>1) Object Classification</h4>
						<!-- <p style="text-align: justify;">
							We collected data from nine rods. The UR5 grasped each rod at its center while it rested on a table; five swept-sine measurements were recorded per grasp before regrasping. The training and test sets contain 100 and 25 samples per object, respectively (900 and 225 in total). For out-of-distribution evaluation, we repeated the collection on new contact surfaces (acrylic sheet, paperback textbook, plastic container) and under new orientations (held in air and ±45° about both horizontal axes). For each out-of-distribution condition, 25 samples were collected per object.
						</p> -->

						<section class="section-ood-media">
						<div class="ood-grid">
							<!-- First image alone = Train Set -->
							<div class="ood-wrapper">
							<div class="ood-set-label">Train Set</div>
							<figure class="ood-card">
								<img src="images/data_collection/plastic_table.png" 
									alt="Plastic table surface" loading="lazy">
								<figcaption>Plastic table</figcaption>
							</figure>
							</div>

							<!-- Grouped = Test Set -->
							<div class="ood-wrapper">
							<div class="ood-set-label">Test Set</div>
							<figure class="ood-card grouped-card">
								<div class="group-row">
								<div>
									<img src="images/data_collection/acrylic_sheet.png" 
										alt="Acrylic sheet surface" loading="lazy">
									<figcaption>Acrylic sheet</figcaption>
								</div>

								<div>
									<img src="images/data_collection/paperback_textbook.png" 
										alt="Paperback textbook surface" loading="lazy">
									<figcaption>Paperback textbook</figcaption>
								</div>

								<div>
									<img src="images/data_collection/plastic_container.png" 
										alt="Plastic container surface" loading="lazy">
									<figcaption>Plastic container</figcaption>
								</div>

								<div>
									<video src="images/data_collection/new_orientation1.mp4"
										autoplay loop muted playsinline preload="auto">
									</video>
									<figcaption>New orientation</figcaption>
								</div>
								</div>
							</figure>
							</div>
						</div>
						</section>
						<br>



						<h4>2) Grasping Position Classification</h4>
						<!-- <p style="text-align: justify;">
							Using the same rods, we labeled grasp locations as edge (1.0&nbsp;cm from the end), quarter (3.2&nbsp;cm), and center (6.3&nbsp;cm). The setup mirrored the object classification protocol: table-resting, five measurements per grasp, and regrasping between sets. We collected 100 training and 25 test samples per position (2,700/675 total). For out-of-distribution tests, we used the same set of new surfaces; for the new-orientation condition, rods were placed upright on a plastic stand.
						</p> -->

						<section class="section-ood-media">
						<div class="ood-grid">
							<!-- First image alone = Train Set -->
							<div class="ood-wrapper">
							<div class="ood-set-label">Train Set</div>
							<figure class="ood-card">
								<img src="images/data_collection/plastic_table.png" 
									alt="Plastic table surface" loading="lazy">
								<figcaption>Plastic table</figcaption>
							</figure>
							</div>

							<!-- Grouped = Test Set -->
							<div class="ood-wrapper">
							<div class="ood-set-label">Test Set</div>
							<figure class="ood-card grouped-card">
								<div class="group-row">
								<div>
									<img src="images/data_collection/acrylic_sheet.png" 
										alt="Acrylic sheet surface" loading="lazy">
									<figcaption>Acrylic sheet</figcaption>
								</div>

								<div>
									<img src="images/data_collection/paperback_textbook.png" 
										alt="Paperback textbook surface" loading="lazy">
									<figcaption>Paperback textbook</figcaption>
								</div>

								<div>
									<img src="images/data_collection/plastic_container.png" 
										alt="Plastic container surface" loading="lazy">
									<figcaption>Plastic container</figcaption>
								</div>

								<div>
									<img src="images/data_collection/new_orientation2.png" 
										alt="New Orientation2" loading="lazy">
									<figcaption>New Orientation</figcaption>
								</div>
								</div>
							</figure>
							</div>
						</div>
						</section>
						<br>

						<h4>3) Pose Estimation from Internal Structure</h4>
						<!-- <p style="text-align: justify;">
							The object was a smooth PLA cylinder containing an asymmetric internal feature. We sampled 18 discrete rotations from 0° to 170° in 10° increments. For each angle, we recorded 100 training and 25 test samples, regrasping every five measurements (1,800/450 total). To evaluate generalization to unseen poses, we introduced an interpolated-pose test set with 16 randomly sampled angles between 0° and 170°
						</p> -->

						<div class="pose-grid">
						<!-- Train -->
						<div class="pose-wrapper">
							<div class="pose-set-label">Train set</div>
							<figure class="ood-card">
							<div class="media-16x9">
								<video src="images/data_collection/internal_structure_train.mp4"
									autoplay muted playsinline webkit-playsinline preload="auto"
									class="sync-poseestimation-video"></video>
							</div>
							<figcaption>10° discretized poses around cylinders (0° to 170°)</figcaption>
							</figure>
						</div>

						<!-- Test -->
						<div class="pose-wrapper">
							<div class="pose-set-label">Test set</div>
							<figure class="ood-card">
							<div class="media-16x9">
								<video src="images/data_collection/internal_structure_test.mp4"
									autoplay muted playsinline webkit-playsinline preload="auto" 
									class="sync-poseestimation-video">
								</video>
							</div>
							<figcaption>In-distribution poses (randomly sampled angles, 0° to 170°)</figcaption>
							</figure>
						</div>
						</div>
						<br>


						<h4>4) Contact Type Classification</h4>
						<!-- <p style="text-align: justify;">
							We defined three contact states for a peg-in-hole task—diagonal (0&nbsp;&lt;&nbsp;θ<sub>z</sub>&nbsp;&lt;&nbsp;90°), line (θ<sub>z</sub>=90°, θ<sub>x</sub>&nbsp;&lt;&nbsp;90°), and in-hole (θ<sub>z</sub>=θ<sub>x</sub>=90°)—using an aluminum rod and a square hole enlarged by +3&nbsp;mm along each axis (≈6° angular tolerance). We gathered 1,000 training and 200 test samples per class. Training poses were discretized on a grid: diagonal states with 45°≤θ<sub>z</sub>≤90° at θ<sub>x</sub>=45°, and line states with θ<sub>z</sub>=90° and 45°≤θ<sub>x</sub>&lt;90°, both at 4.5° increments. We additionally evaluated on interpolated poses within the training ranges (20 random diagonal and 20 random line cases) and on out-of-distribution diagonal poses sampled from 40.5°≤θ<sub>x</sub>≤171° and 9°≤θ<sub>z</sub>&lt;90°.
						</p> -->

						<section class="section-ood-media contact-ood">
							<div class="ood-grid">
								<!-- First image alone = Train Set -->
								<div class="ood-wrapper">
									<div class="ood-set-label">Train Set</div>
									<figure class="ood-card">
										<video src="images/data_collection/hard_coded_trajectory.mp4"
											autoplay muted playsinline webkit-playsinline preload="auto" 
											class="sync-contacttype-video">
										</video>
										<figcaption>A handcrafted trajectory to the in-hole state</figcaption>
									</figure>
								</div>

								<!-- Grouped = Test Set -->
								<div class="ood-wrapper">
									<div class="ood-set-label">Test Set</div>
									<figure class="ood-card grouped-card">
										<div class="group-row">
											<div>
												<video src="images/data_collection/in_distribution.mp4"
													autoplay muted playsinline webkit-playsinline preload="auto" 
													class="sync-contacttype-video">
												</video>
												<figcaption>In-Distribution Pose: <br>
													θx = 45°, θz = 45°</figcaption>
											</div>

											<div>
												<video src="images/data_collection/interpolated.mp4"
													autoplay muted playsinline webkit-playsinline preload="auto" 
													class="sync-contacttype-video">
												</video>
												<figcaption>Interpolated Poses: <br>
													θx = 45°, 45° ≤ θz ≤ 90°</figcaption>
											</div>

											<div>
												<video src="images/data_collection/out_of_distribution.mp4"
													autoplay muted playsinline webkit-playsinline preload="auto" 
													class="sync-contacttype-video">
												</video>
												<figcaption>Out-of-Distribution Poses: 
													40.5° ≤ θx ≤ 81°, 9° ≤ θz ≤ 90°</figcaption>
											</div>
										</div>
									</figure>
								</div>
							</div>
						</section>





						<hr/>
						<h3>Task Learning</h3>
						<p	style="text-align: justify;">
							The contact type classifier was used as the sole feedback signal for a long-horizon peg insertion task. We built a simulator with ground-truth labels and trained an imitation learning policy that incorporated observation history to handle classifier uncertainty. The learned policy achieved a 95% success rate in simulation and transferred to the real robot. In UR5 robot rollouts, we achieved success rates of 90% in-distribution and 60% out-of-distribution.
						</p>

						<div style="text-align: center;">
						<img src="images/peg_in_hole_task/acoustic-task-learning.png" 
							alt="task_learning" loading="lazy" style="max-width: 600px; height: auto; display: block; margin: 0 auto;">
						<figcaption>Learning task-specific classifiers and policy derivation</figcaption>
						</div>

						<div class="robustness-videos" style="text-align: center;">
						<figure style="display: inline-block; margin: 0;">
							<video src="images/peg_in_hole_task/peg_insertion_task.mp4"
								autoplay loop muted playsinline preload="auto"
								style="max-width: 800px; height: auto; display: block; margin: 0 auto;">
							</video>
							<figcaption style="text-align: center; margin-top: 6px;">
							Peg-insertion Task
							</figcaption>
						</figure>
						</div>




						<hr/>
						<h3>Sensing Robustness</h3>
						<p	style="text-align: justify;">
							We evaluate robustness by adding background music at 75 dB, where the contact type classifier sustains 87% accuracy, indicating resilience to acoustic and vibrational disturbances.
						</p>

						<div class="robustness-videos" id="robustness-sync">
						<video src="images/sensing_robustness/C0018.mp4" autoplay loop muted playsinline preload="auto" class="sync-robustness-video"></video>
						<video src="images/sensing_robustness/C0020.mp4" autoplay loop muted playsinline preload="auto" class="sync-robustness-video"></video>
						<video src="images/sensing_robustness/C0027.mp4" autoplay loop muted playsinline preload="auto" class="sync-robustness-video"></video>
						</div>




						<hr/>
						<h3>Acknowledgements</h3>
						<p>We thank Pedro Piacenza, Trey Smith, Brian Coltin, Peter Ballentine, and Ye Zhang for insightful discussions and support. We thank <a href="https://roamlab.github.io/hwasp/">Hardware as Policy</a> and <a href="https://umi-gripper.github.io/">UMI</a> for website inspiration. This  work  was  supported  by  a  NASA  Space  Technology  Graduate  Research 
Opportunity.</p>
						



						<hr/>
				        <h3>Contact</h3>
				        <p>If you have any questions, please feel free to contact <a href="http://www.ricandrobots.com/">Eric Chang</a> or <a href="http://zhanpenghe.github.io/">Zhanpeng He</a></p>
						<hr/>

					</section>

			</div>

		<!-- Footer -->
			<footer id="footer">
				<div class="inner">
					<ul class="copyright">
						<li>Meet <a href="https://en.wikipedia.org/wiki/Danbo_(character)">Danbo</a> the cardboard robot.</li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.poptrox.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>

			<script>
			document.addEventListener("DOMContentLoaded", () => {
			// Give the 3-video block an id in your HTML for a clean hook:
			// <div class="robustness-videos" id="robustness-sync"> ... </div>
			const container = document.getElementById("robustness-sync");
			if (!container) return;

			const videos = Array.from(container.querySelectorAll("video"));

			// We’ll control the looping ourselves
			videos.forEach(v => v.removeAttribute("loop"));

			let finished = 0;

			// When ALL finish, restart them together
			const onEnded = () => {
				finished++;
				if (finished === videos.length) {
				// Reset and play in sync
				videos.forEach(v => { v.currentTime = 0; });
				// A microtask helps them fire together
				Promise.resolve().then(() => videos.forEach(v => v.play()));
				finished = 0;
				}
			};

			videos.forEach(v => v.addEventListener("ended", onEnded));

			// Ensure first round starts together after they’re loaded enough
			// (they already have autoplay+muted+playsinline+preload="auto")
			const ready = v =>
				v.readyState >= 2 || v.buffered.length > 0 || v.seeking === false;

			const startTogether = () => {
				if (videos.every(ready)) {
				videos.forEach(v => v.currentTime = 0);
				videos.forEach(v => v.play());
				} else {
				setTimeout(startTogether, 50);
				}
			};
			startTogether();
			});
			</script>

			<script>
			document.addEventListener("DOMContentLoaded", () => {
			const videos = document.querySelectorAll(".sync-poseestimation-video");
			let loadedCount = 0;

			// Ensure all videos are loaded before playing
			videos.forEach(video => {
				video.addEventListener("loadeddata", () => {
				loadedCount++;
				if (loadedCount === videos.length) {
					playAll();
				}
				});

				// Stop each video when it ends
				video.addEventListener("ended", () => {
				checkAllEnded();
				});
			});

			function playAll() {
				videos.forEach(v => {
				v.currentTime = 0;
				v.play();
				});
			}

			function checkAllEnded() {
				if ([...videos].every(v => v.ended)) {
				setTimeout(() => playAll(), 200); // restart all together after short delay
				}
			}
			});
			</script>

			<script>
			document.addEventListener("DOMContentLoaded", () => {
			const videos = document.querySelectorAll(".sync-contacttype-video");
			let loadedCount = 0;

			// Ensure all videos are loaded before playing
			videos.forEach(video => {
				video.addEventListener("loadeddata", () => {
				loadedCount++;
				if (loadedCount === videos.length) {
					playAll();
				}
				});

				// Stop each video when it ends
				video.addEventListener("ended", () => {
				checkAllEnded();
				});
			});

			function playAll() {
				videos.forEach(v => {
				v.currentTime = 0;
				v.play();
				});
			}

			function checkAllEnded() {
				if ([...videos].every(v => v.ended)) {
				setTimeout(() => playAll(), 200); // restart all together after short delay
				}
			}
			});
			</script>

			<script>
			document.addEventListener("DOMContentLoaded", () => {
			const videos = document.querySelectorAll(".sync-robustness-video");
			let loadedCount = 0;

			// Ensure all videos are loaded before playing
			videos.forEach(video => {
				video.addEventListener("loadeddata", () => {
				loadedCount++;
				if (loadedCount === videos.length) {
					playAll();
				}
				});

				// Stop each video when it ends
				video.addEventListener("ended", () => {
				checkAllEnded();
				});
			});

			function playAll() {
				videos.forEach(v => {
				v.currentTime = 0;
				v.play();
				});
			}

			function checkAllEnded() {
				if ([...videos].every(v => v.ended)) {
				setTimeout(() => playAll(), 200); // restart all together after short delay
				}
			}
			});
			</script>


	</body>
</html>
